---
title: Flow Matching 与 DDIM 小记
tags:
  - Flow Matching
  - DDIM
  - 生成模型
  - 炼丹
tikzjax: true
header-includes: |
  \usepackage{amsmath}
  \usetikzlibrary{positioning, shapes.geometric, arrows.meta}
---

## 背景故事

2022 年 10 月左右，同期有三篇关于 Flow 模型用于图像生成的论文发表，并都被 ICLR 2023 录用：

- [Flow Matching for Generative Modeling](https://arxiv.org/abs/2210.02747)
- [Learning to Generate and Transfer Data with Rectified Flow](https://arxiv.org/abs/2209.03003)
- [Building Normalizing Flows with Stochastic Interpolants](https://arxiv.org/pdf/2209.15571)

这三篇文章从不同角度出发得推导出了同一个生成模型，下面再整理一下三篇论文的故事讲法。

### Flow Matching

{% folding 背景：连续归一化流 %}

连续归一化流（Continuous Normalizing Flows, CNF）的概念最早由 [Chen et al. (2018)](https://arxiv.org/abs/1806.07366) 提出，CNF 通过求解常微分方程（ODE）来实现数据的变换。时间依赖的概率密度路径 $p_t(x)$ 满足 $\int p_t(x) \d x = 1$, 流的动态由时间依赖的向量场 $\b{v}_t:[0,1]\times\R^d\to\R^d$ 定义：

$$
\frac{\d}{\d t}\phi_t(\b{x})=\b{v}_t(\phi_t(\b{x}),\quad \phi_0(\b{x})=\b{x}
$$

CNF 可以通过推前操作将先验分布 $p_0$ 转换为目标分布 $p_t$（理论上的结论，并不能用于采样）：

$$
p_t(\b{x}) = p_0(\phi_t^{-1}(\b{x})) \det\left[ \frac{\p \phi_t^{-1}(\b{x})}{\p \b{x}} \right]
$$

同时连续性方程可用于验证向量场 $\b{v}_t$ 是否生成满足以上性质的概率路径：

$$
\frac{\d}{\d t} p_t(\b{x}) + \nabla \cdot (p_t(\b{x}) \b{v}_t(\b{x})) = 0
$$

{% endfolding %}

CNF 可以用于建模任意的概率路径，然而此前不存在高效的方法 [^1]来学习向量场 $\b{v}_t$。Flow Matching 论文提出了条件流匹配方法（Conditional Flow Matching, CFM）实现从数据集中采样学习向量场 $\b{v}_t$。首先引入 Flow Matching 损失函数，用神经网络 $\b{v}_t(\b{x};\theta)$ 来回归近似向量场 $\b{u}_t(\b{x})$：

$$
\mathcal{L}_{\text{FM}}(\theta) = \mathbb{E}_{t,p_t(\b{x})}\left\|\b{v}_t(\b{x};\theta) - \b{u}_t(\b{x})\right\|^2
\label{fmLoss}
$$

[^1]: 关于 CNF 的对数似然求解方法可以参考 CNF 原文。[这篇知乎文章](https://zhuanlan.zhihu.com/p/10764455963) 也有介绍。对数似然训练 Neural ODE 在高维空间效果差且不稳定。

再来进一步考虑如何构造可从数据集中采样求得的 $\b{u}_t$ 和 $p_t(\b{x})$。将数据集中的样本 $\b{x}_1\sim q(\b{x}_1)$ 视作条件，那么 $p_t(\mathbf{x})$ 作为条件概率路径 $p_t(\b{x}|\b{x}_1)$ 的边缘分布可以通过积分得到：

$$
p_t(\b{x}) = \int p_t(\b{x}|\b{x}_1) q(\b{x}_1) \d \b{x}_1
$$

类似的，向量场 $\b{u}_t(\b{x})$ 可以视作条件向量场 $\b{u}_t(\b{x}|\b{x}_1)$ 的边缘分布，通过积分得到:

$$
\b{u}_t(\b{x}) = \int \b{u}_t(\b{x}|\b{x}_1) \frac{p_t(\b{x}|\b{x}_1) q(\b{x}_1)}{p_t(\b{x})} \d \b{x}_1
$$

原文证明了条件向量场 $\b{u}_t(\b{x}|\b{x}_1)$ 确实能生成条件概率密度路径 $p_t(\b{x}|\b{x}_1)$。进一步，论文证明了条件流匹配损失函数

$$
\mathcal{L}_{\text{CFM}}(\theta) = \mathbb{E}_{t,q_(\b{x}_1),p_t(\b{x}|\b{x}_1)}\left\|\b{v}_t(\b{x};\theta) - \b{u}_t(\b{x}|\b{x}_1)\right\|^2
$$

的梯度等价于 FM 损失函数 $(\ref{fmLoss})$ 的梯度，故可以使用 CFM 损失函数训练 CNF 模型。此结论并未依赖于具体的条件概率路径 $p_t(\b{x}|\b{x}_1)$ 或条件向量场 $\b{u}_t(\b{x}|\b{x}_1)$ 的设计。

在图像生成模型中，一般把高斯噪声作为先验分布，即 $p(\b{x})=\mathcal{N}(\b{x};\b{0},\b{I})$。流映射在 $\b{x}_1$ 条件下总是可以写成

$$
\phi_t(\b{x}) = \sigma_t(\b{x}_1)\b{x} + \mu_t(\b{x}_1)
$$

可以证明此时的条件速度场具有唯一的形式

$$
\b{u}_t(\b{x}|\b{x}_1) = \frac{\sigma_t'(\b{x}_1)}{\sigma_t(\b{x}_1)}(\b{x} - \mu_t(\b{x}_1)) + \mu_t'(\b{x}_1)
$$

DDIM 的均值 $\mu_t(\b{x}_1)=\alpha_{1-t}\b{x}_1$ 和方差 $\sigma_t(\b{x}_1)=\sqrt{1-\alpha_{1-t}^2}$ 可以直接带入到上式中，得到 Flow Matching 框架下的条件速度场。

论文又提出 OT 速度场的设计，使用线性插值的均值 $\mu_t(\b{x}_1)=t\b{x}_1$ 和方差 $\sigma_t(\b{x}_1)=1-(1-\sigma_{\text{min}})t$。通过实验发现 OT 速度场训练收敛更快且 FID 更低。

### Rectified Flow

{% folding 背景：传输映射问题 %}

给定 $\R^d$ 上两个可以观测的分布 $\pi_0,\pi_1$，目标是寻找传输映射 $T:\R^d\to\R^d$ 将 $Z_0\sim\pi_0$ 映射到 $Z_1=T(Z_0)\sim\pi_1$。

在生成建模中，$\pi_1$ 是未知但可观测目标分布（图像数据集），而 $\pi_0$ 是简单的先验分布（如高斯分布）。还可以考虑传输建模，即 $\pi_0$ 和 $\pi_1$ 都是未知的可观测分布。

最优传输（Optimal Transport, OT）目标希望最小化某个代价函数 $c:\R^d\to\R$，即

$$
\min_{T} \mathbb{E}\left[c(T(Z_0)-Z_0)\right]
$$

其中 $c$ 可以是 L2 范数或则其他度量。一般的 OT 问题非常难解，在图像生成任务中也不太关心最小化代价。Rectified Flow 和 Reflow 是此背景下一种实用的近似方法。

{% endfolding %}

Rectified Flow 使用一个简单的 ODE 来学习传输映射 $T$。从 $Z_0\sim\pi_0$ 出发，沿速度场 $v:\R^d\times[0,1]\to\R^d$ 进行演化，得到 $Z_1\sim\pi_1$：

$$
\d Z_t = v_t(Z_t)\d t,\quad t\in[0,1],Z_0\sim\pi_0
\label{rfOde}
$$

问题转换为学习速度场 $v$。直观的想法（先前的工作）是想最小化 ODE 采样结果 $Z_1$ 的分布 $\rho_1^v$ 和目标分布 $\pi_1$ 的距离 $D(\rho_1^v,\pi_1)$，如 KL 散度，但多次模拟求解 ODE 的代价太高了。发现此问题中，速度场 $v$ 实际上是过参数化的：我们只关心开头和结尾的分布应当是 $\pi_0$ 和 $\pi_1$，而中间的分布并不重要，可以人为地设置非常强的先验。简单的选择是让中间分布是开头结尾的某种线性插值，走直线路径，这样不仅能更接近 OT 目标，还能减少 ODE 的求解次数。

对源分布和目标分布中观测到的样本 $X_0\sim\pi_0$ 和 $X_1\sim\pi_1$，定义线性插值

$$
X_t = (1-t)X_0 + tX_1,\quad t\in[0,1]
$$

则 $X_t$ 满足以下 ODE：

$$
\d X_t = (X_1-X_0)\d t
$$

还不能直接用这个 ODE 作为速度场 $v$，因为它不是因果的，$X_t$ 依赖于最终状态 $X_1$。论文提出 Rectify 过程来因果化速度场 $v$。思路如下：

![Rectify 过程的 Rewire 特点](https://img.duanyll.com/img/97c80806.png)

1. 不同的插值轨迹 $\{X_t\}$ 之间可能存在相交点，在同一个 $X_t$ 处会有多个可能的 $\dot{X}_t$ 取值，因为不知道要沿哪条轨迹走。
2. 另一方面，在 ODE $(\ref{rfOde})$ 中，$\dot{Z}_t$ 必须由 $Z_t$ 唯一确定，于是不同的 $\{Z_t\}$ 轨迹是不能相交的。
3. 于是在相交点处，$v_t(X_t)$ 可以取值为 $\dot{X}_t$ 在所有可能的 $\dot{X}_t$ 取值上的平均值。
   $$
   v_t(X_t)=\mathbb{E}[\dot{X}_t|X_t]
   \label{rf-velocity}
   $$
4. 导致原本的直线轨迹 $\{X_t\}$ 必须在相交点处弯曲来避免相交，并改变原分布和目标分布的映射关系。

以上 Rectify 过程更新了传输映射 $T$，但可证明保持了边缘分布 $\pi_0$ 和 $\pi_1$ 不变。训练目标为

$$
\min_v\int_0^1\mathbb{E}\left[\left\|\dot{X}_t-v_t(X_t)\right\|^2\right]\d t
$$

代入线性插值轨迹得到训练目标

$$
\min_v\int_0^1\mathbb{E}\left[\left\|X_1-X_0-v_t((1-t)X_0+tX_1)\right\|^2\right]\d t
$$

> 此训练目标就可以直接用在代码里了。

Rectify 过程将 $\{X_t\}$ 轨迹转换为因果的 $\{Z_t\}$ 轨迹，不仅保持边缘分布不变，还可以证明对于凸的代价函数 $c$，传输代价不增

$$
\mathbb{E}[c(Z_1-Z_0)]\leq\mathbb{E}[c(X_1-X_0)]
$$

{% folding Reflow %}

![Reflow 拉直轨迹](https://img.duanyll.com/img/5856eba6.png)

原本 $\{X_t\}$ 轨迹是直的，经过 Rectify 过程后 $\{Z_t\}$ 会在相交点弯曲，起不到一开始选择线性插值轨迹时，保持直线的优势。论文提出 Reflow 过程来修正这个问题，使用 Rectify 过程得到的 $\{Z_0,Z_1\}$ 配对耦合重新训练速度场 $v$ 并反复迭代：

$$
\{Z_t^{k+1}\}=\operatorname{Rectify}(\operatorname{Interp}(Z_0^k,Z_1^k))
$$

论文证明了对于下面的 “直线程度” 度量

$$
S(\{Z_t\})=\int_0^1\mathbb{E}\left[\left\|Z_1-Z_0-\dot{Z}_t\right\|^2\right]\d t
$$

满足

$$
\mathbb{E}_{k\in\operatorname{Uniform}\{1,\cdots,K\}}[S(\{Z_t^k\})]=\mathcal{O}(1/K)
$$

随 $k\to+\infty$ 收敛到 0。也就是说，经过 Reflow 过程后，$\{Z_t\}$ 轨迹会越来越接近直线。

Reflow 其实就是从 OT 角度解释了先前的时间步蒸馏技巧。常见的预训练模型都没有做 Reflow 蒸馏。

{% endfolding %}

### Stochastic Interpolants

WIP.

### Diffusion

DDPM 和 DDIM 也有很多种推导的方法，下面摘自 [苏神博客](https://spaces.ac.cn/archives/9164) 中贝叶斯角度的推导，比较接近 DDIM 论文的推导方式。

#### DDPM

与 GAN 类似，DDPM 模型将生成定义为将一个随机噪声 $z$ 变换成真实图像 $x$ 的过程。然而，像 GAN 一样一步将噪声映射到真实图像的分布是困难的，因此 DDPM 考虑逐步地将噪声 $z$ 变换为真实图像。首先定义一个前向过程，即将真实图像样本 $x$ 逐步添加噪声，直到完全成为高斯噪声，则生成图像的过程即为将噪声 $z$ 逐步变为样本数据的反向过程。前向过程的每个步骤可以表示为

$$
    x_t = \alpha_tx_{t-1} + \beta_t\epsilon_t,\quad\epsilon_t\sim\mathcal{N}(0, I),\quad t=1,2,\cdots,T,
$$

其中 $x_0=x$ 是真实图像样本数据，$x_T=z$ 是完全随机的高斯噪声，$T$ 是设置的时间步数。$\{\alpha_t\}$ 和 $\{\beta_t\}$ 是控制每一步添加噪声强度的系数。前向过程也可以写成条件概率分布的形式

$$
    p(x_t|x_{t-1}) \sim \mathcal{N}(x_t;\alpha_tx_{t-1}, \beta_t^2I),
    \label{eq:pxtxtm1}
$$

其中 $p(x_t|x_{t-1})$ 代表在前向过程中 $x_{t-1}$ 到 $x_t$ 的条件概率分布。

注意到如果规定 $\alpha_t^2+\beta_t^2=1$，则可以从 $x_0$ 直接推导出 $x_t$

$$
    \begin{aligned}
        x_t &= \alpha_tx_{t-1} + \beta_t\epsilon_t \\
            &= \alpha_t(\alpha_{t-1}x_{t-2} + \beta_{t-1}\epsilon_{t-1}) + \beta_t\epsilon_t \\
            &= \left(\prod_{i=1}^t\alpha_i\right)x_0 + \sum_{i=1}^t\left(\prod_{j=i+1}^t\alpha_j\right)\beta_i\epsilon_i,
    \end{aligned}
$$

于是 $x_t$ 中包含 $t$ 个独立的高斯噪声 $\epsilon_i$ 的线性组合。代入 $\alpha_t^2+\beta_t^2=1$ 的条件，得到

$$
\prod_{i=1}^t\alpha_i^2+\sum_{i=1}^t\left(\prod_{j=i+1}^t\alpha_j\right)^2\beta_i^2=1.
$$

将 $\prod_{i=1}^t\alpha_i$ 记为 $\bar{\alpha_t}$，$\sqrt{1-\prod_{i=1}^t\alpha_i}$ 记为 $\bar{\beta_t}$\footnote{本论文中为了符号的一致性，$\bar{\alpha}_t$ 的定义与 DDPM 论文并不相同。}，则 $x_t$ 关于 $x_0$ 的条件概率分布为

$$
    p(x_t|x_0)=\mathcal{N}(x_t;\bar{\alpha_t}x_0, \bar{\beta_t}^2I),
    \label{eq:pxtx0}
$$

也可以写成

$$
    x_t = \bar{\alpha_t}x_0 + \bar{\beta_t}\bar{\epsilon}_t,\quad\bar{\epsilon}_t\sim\mathcal{N}(0, I).
    \label{eq:xtx0e}
$$

只要 $\lim_{t\to\infty}\bar{\alpha_t}=0$，经过充分多的时间步后，$x_t$ 就会足够接近标准高斯分布 $\mathcal{N}(0, I)$。利用 $(\ref{eq:pxtx0})$，可以快速从 $x_0$ 采样到 $x_t$。接下来考虑反向过程，关键在于求条件概率分布 $p(x_{t-1}|x_t)$ 以便从噪音多的时间步还原噪音少的时间步。根据贝叶斯公式

$$
    p(x_{t-1}|x_t)=\frac{p(x_t|x_{t-1})p(x_{t-1})}{p(x_t)},
$$

然而，$p(x_{t-1})$ 和 $p(x_t)$ 是未知的。考虑引入 $x_0$ 作为条件变量，则

$$
    p(x_{t-1}|x_t,x_0)=\frac{p(x_t|x_{t-1})p(x_{t-1}|x_0)}{p(x_t|x_0)}.
    \label{eq:ddpm-bayes}
$$

代入 $(\ref{eq:pxtxtm1})$ 和 $(\ref{eq:pxtx0})$，可以写出 $p(x_{t-1}|x_t,x_0)$ 服从的分布

$$
    p(x_{t-1}|x_t,x_0) \sim \mathcal{N}\left(
        x_{t-1};
        \frac{\alpha_t\bar{\beta}^2_{t-1}}{\bar{\beta}^2_t}x_t
        +\frac{\bar{\alpha}_{t-1}\beta_t^2}{\bar{\beta}_t^2}x_0,
        \frac{\bar{\beta}_{t-1}^2\beta_t^2}{\bar{\beta}_t^2}I
    \right),
    \label{eq:ddpm-bayes-sol}
$$

使用此分布采样 $x_{t-1}$ 需要已知 $x_0$，然而 $x_0$ 正是最终要生成的图像。DDPM 引入可学习的函数 $\bar{\mu}(x_t)$ 来近似估计未知的 $x_0$，训练的目标为最小化损失函数 $\|x_0-\bar{\mu}(x_t)\|^2$。DDPM 实际上没有使用神经网络 $\varepsilon_\theta(x_t,t)$ 直接预测原始图像 $x_0$，而是用于预测添加的噪声 $\bar{\epsilon}_t$，这样在每一步上神经网络的输出都服从标准高斯分布，有助于提高训练的稳定性。根据 $(\ref{eq:xtx0e})$，可以重参数化 $\bar{\mu}(x_t)$ 为

$$
    \bar{\mu}(x_t) = \frac{1}{\bar{\alpha}_t}(x_t-\bar{\beta}_t\varepsilon\theta(x_t,t)),
    \label{eq:muxt}
$$

进而得到完整的损失函数

$$
    \mathcal{L}(\theta) = \mathbb{E}_{t,x_0,\epsilon_t}\left\|
        \epsilon_t-\varepsilon_\theta(\bar{\alpha}_t x_0 + \bar{\beta}_t\epsilon_t,t)
    \right\|^2.
    \label{eq:ddpm-loss}
$$

最后，用 $(\ref{eq:muxt})$ 近似 $x_0$ 代入 $(\ref{eq:ddpm-bayes-sol})$，得到与 $x_0$ 无关的反向过程条件概率分布

$$
    p(x_{t-1}|x_t) \sim \mathcal{N}\left(
        x_{t-1};
        \frac{1}{\alpha_t}\left(
            x_t-\frac{\beta_t^2}{\bar{\beta}_t}\varepsilon_\theta(x_t,t)
        \right),
        \frac{\bar{\beta}_{t-1}^2\beta_t^2}{\bar{\beta}_t^2}I
    \right),
$$

并进一步得到 DDPM 的迭代采样公式

$$
    x_{t-1} = \frac{1}{\alpha_t}\left(
        x_t-\frac{\beta_t^2}{\bar{\beta}_t}\varepsilon_\theta(x_t,t)
    \right)+\sigma_t\epsilon,\quad\epsilon\sim\mathcal{N}(0, I),
    \label{eq:ddpm-step}
$$

其中 $\sigma_t=\bar{\beta}_{t-1}\beta_t/\bar{\beta}_t$，$t=1,\cdots,T$。

在 DDPM 原始的实验中，$T$ 取值为 1000，$\alpha_t$ 根据经验设置为 $\alpha_t=\sqrt{1-\frac{0.02t}{T}}$. 观察 $(\ref{eq:ddpm-step})$，可以发现每一步的采样都需要计算神经网络 $\varepsilon_\theta(x_t,t)$，因此计算量非常大。同时 DDPM 的每一步采样都包括对随机噪声 $\epsilon$ 的采样，结果具有不确定性。

#### DDIM

DDIM（Denoising Diffusion Implicit Models）模型在 DDPM 的基础上进一步推导了采样公式，发展了允许跳过时间步和确定性的采样方法，并允许将扩散模型的采样过程与常微分方程和随机微分方程的求解算法结合起来。本节将介绍 DDIM 的主要推导过程和结论。

注意到在 DDPM 的神经网络 $\varepsilon_\theta(x_t,t)$ 的训练过程 $(\ref{eq:ddpm-loss})$ 中，只依赖了关于 $p(x_t|x_0)$ 的假定 $(\ref{eq:pxtx0})$，而与具体的 $p(x_t|x_{t-1})$ 的设置 $(\ref{eq:pxtxtm1})$ 无关，因此在求解贝叶斯公式 $(\ref{eq:ddpm-bayes})$ 时，不再代入 $p(x_t|x_{t-1})$，而是假设 $p(x_{t-1}|x_t,x_0)$ 服从高斯分布

$$
    p(x_{t-1}|x_t,x_0) \sim \mathcal{N}(x_{t-1};\kappa_t x_t+\lambda_t x_0, \sigma^2_t I),
$$

则关于 $x_{t-1}$ 的采样公式可以写成

$$
    \begin{aligned}
        x_{t-1} &= \kappa_t x_t + \lambda_t x_0 + \sigma_t\epsilon \\
                &= \kappa_t(\bar{\alpha}_t x_0+\bar{\beta}_t\epsilon_1) + \lambda_t x_0 + \sigma_t\epsilon_2 \\
                &= (\kappa_t\bar{\alpha}_t+\lambda_t)x_0 + \kappa_t\bar{\beta}_t\epsilon_1 + \sigma_t\epsilon_2 \\
                &= (\kappa_t\bar{\alpha}_t+\lambda_t)x_0 + \sqrt{\kappa_t\bar{\beta}_t^2+\sigma^2_t}\epsilon.
    \end{aligned}
$$

比较上式与 $(\ref{eq:xtx0e})$ 的系数，消去 $\kappa_t$ 和 $\lambda_t$，得到

$$
    p(x_{t-1}|x_t,x_0) \sim \mathcal{N}\left(
        x_{t-1};
        \frac{\sqrt{\bar{\beta}^2_{t-1}-\sigma_t^2}}{\bar{\beta}_t}x_t
        +\left(\bar{\alpha}_{t-1}-\bar{\alpha}_t\frac{\sqrt{\bar{\beta}^2_{t-1}-\sigma_t^2}}{\bar{\beta}_t}\right)x_0,
        \sigma^2_t I
    \right),
    \label{eq:ddim-bayes-sol}
$$

是关于 $p(x_{t-1}|x_t,x_0)$ 服从的分布的通解，具有自由参数 $\sigma_t$. 沿用 DDPM 训练的神经网络模型，用 $(\ref{eq:muxt})$ 近似 $x_0$ 代入 $(\ref{eq:ddim-bayes-sol})$，得到 DDIM 的迭代采样公式

$$
    x_{t-1} = \frac{x_t-\bar{\beta}_t\varepsilon_\theta(x_t,t)}{\alpha_t}
            + \sqrt{\bar{\beta}^2_{t-1}-\sigma^2_t}\varepsilon_\theta(x_t,t)
            + \sigma_t\epsilon,\quad\epsilon\sim\mathcal{N}(0, I),
    \label{eq:ddim-step}
$$

公式中的三项可分别视作从 $x_t$ 预测的 $x_0$，从当前步指向 $x_t$ 的方向和随机噪声。注意到 $\sigma_t$ 是可自由选取的超参数，当 $\sigma_t=0$ 时，$(\ref{eq:ddim-step})$ 变为确定性的采样方式

$$
    x_{t-1} = \frac{x_t-\bar{\beta}_t\varepsilon_\theta(x_t,t)}{\alpha_t}
            + \bar{\beta}_{t-1}\varepsilon_\theta(x_t,t).
    \label{eq:ddim-step-deterministic}
$$

另外，在推导 $(\ref{eq:ddim-step})$ 的过程中并不依赖于 $p(x_{t-1}|x_t,x_0)$，同样的过程也适用于推导任意两个时间步 $t$ 和 $t'$ 之间的采样公式

$$
    x_{t'} = \frac{\bar{\alpha}_{t'}(x_t-\bar{\beta}_t\varepsilon_\theta(x_t,t))}{\bar{\alpha}_t}
            + \sqrt{\bar{\beta}^2_{t'}-\sigma^2_t}\varepsilon_\theta(x_t,t)
            + \sigma_t\epsilon,\quad\epsilon\sim\mathcal{N}(0, I),
$$

这实质上允许了以任意的时间步序列 $\{\tau_i\}$ 进行采样，不仅可以跳过时间步节约计算量，还允许确定性地从图像样本 $x_0$ 完全或者部分地反推对应的带噪声图像 $x_t$，为带参考图像作为条件的图像生成和编辑提供了注入参考图像信息的方法，称为 DDIM 反转。$(\ref{eq:ddim-step})$ 也可以视作使用欧拉法求解常微分方程，可以推导出对应的常微分方程形式或者随机微分方程的形式，然后使用 Heun 法、Runge-Kutta 法等效率更高的数值求解方法进行采样。

## Flow 与 DDIM 联系

三篇提出 Flow 模型的论文都分析了 DDIM 模型在 Flow 框架中的地位。在 FM 论文中，可以使用 DDIM 的均值和方差特性来设计条件速度场 $\b{u}_t(\b{x}|\b{x}_1)$，从而得到 Flow Matching 的条件流匹配损失函数 $\mathcal{L}_{\text{CFM}}$。在 Rectified Flow 框架和 Stochastic Interpolants 框架中，DDIM 相当于使用球面插值取代了线性插值，并应用了时间步缩放。

后续有一些博客文章整理和可视化了 Flow Matching 和 DDIM 的关系：

- [Diffusion Meets Flow Matching](https://diffusionflow.github.io/) - DeepMind
- [Let us Flow Together](https://rectifiedflow.github.io/) - UT Austin

Flow 和 Diffusion 的关系可以从重参数化、插值方式和 ODE/SDE 的角度来讨论。

### 重参数化

此处重参数化指的是在模型中神经网络预测的对象不一样。DDIM 采样公式可以写成以下简单形式：

$$
Z_s = \alpha_s\hat{X}+\sigma_s\hat{\epsilon},\quad \hat{X}=\frac{Z_t-\sigma_t\hat{\epsilon}}{\alpha_t}
$$

其中 $\hat{X}$ 是从当前步对最终干净图像的预测，$\hat{\epsilon}$ 是当前步对噪声的预测。一般 Diffusion 模型中神经网络预测的是 $\hat{\epsilon}$，也可以选择预测 $\hat{X}$。

Flow 模型的采样公式为

$$
Z_s = Z_t + \hat{u} \cdot (s-t)
$$

神经网络预测速度场 $\hat{u}$。于是发现无论神经网络预测的是 $\hat{X}$，$\hat{\epsilon}$ 还是 $\hat{u}$，采样公式都可以统一地写成

$$
\tilde{Z}_s = \tilde{Z}_t + \phi(Z_t,t)\cdot(\eta_s-\eta_t)
$$

其中 $\theta$ 是神经网络，$\tilde{Z}_t$ 和 $\eta_t$ 是

| $\theta$         | $\tilde{Z}_t$             | $\eta_t$                       |
| ---------------- | ------------------------- | ------------------------------ |
| $\hat{X}$        | $Z_t/\sigma_t$            | $\alpha_t/\sigma_t$            |
| $\hat{\epsilon}$ | $Z_t/\alpha_t$            | $\sigma_t/\alpha_t$            |
| $\hat{v}$        | $Z_t/(\alpha_t+\sigma_t)$ | $\sigma_t/(\alpha_t+\sigma_t)$ |
| $\hat{u}$        | $Z_t$                     | $t$                            |

表中 $\hat{v}$ 指一般的 Flow Matching 模型，$\hat{u}$ 特指 Rectified Flow 模型的线性插值速度场。

> 此处跳过 DeepMind 博客中关于采样时重参数化和缩放的讨论，在下一小节讨论插值时会提到。

考虑重参数化对训练过程的影响。常用的 DDIM 的损失函数为

$$
\mathcal{L}_{\text{DDIM}} = \mathbb{E}_{t,x,\epsilon}\left[w(\lambda_t)\cdot\frac{\d\lambda}{\d t}\cdot\|\hat{\epsilon}-\epsilon\|^2\right],\quad\lambda_t=\ln\frac{\alpha_t^2}{\sigma_t^2}
$$

其中 $\lambda_t$ 是对数信噪比，$w(\lambda_t)$ 是人为选定的时间步权重函数。

在 Stable Diffusion 3 中，CFM 训练的损失函数中也引入了人为选择的时间步权重函数：

$$
\mathcal{L}_{\text{SD3}} = \mathbb{E}_{t,x,\epsilon}\left[w(\lambda_t)\cdot\|\hat{u}-u\|^2\right]
$$

为了便于比较，我们把这些损失函数都重参数化使用 $\hat{\epsilon}$ 来表达

| $\theta$                                         |                                                 $\mathcal{L}$ | 说明                                             |
| :----------------------------------------------- | ------------------------------------------------------------: | ------------------------------------------------ |
| $\hat{\epsilon}$                                 |                               $\|\hat{\epsilon}-\epsilon\|^2$ | $\lambda$ 小时，过于关注 $\hat{\epsilon}$ 的误差 |
| $\hat{X}=(Z_t-\sigma_t\hat{\epsilon})/\alpha_t$  |                 $e^{-\lambda_t}\|\hat{\epsilon}-\epsilon\|^2$ | $\lambda$ 大时对 $\epsilon$ 的误差不敏感         |
| $\hat{v}=\alpha_t\hat{\epsilon}-\sigma_t\hat{X}$ | $\alpha_t^2(e^{-\lambda_t}+1)^2\|\hat{\epsilon}-\epsilon\|^2$ |
| $\hat{u}=\hat{\epsilon}-\hat{X}$                 |           $(e^{-\lambda/2}+1)^2\|\hat{\epsilon}-\epsilon\|^2$ | 在两种情形之间取得平衡                           |

表明神经网络的不同预测对象意味着在预测 $\hat{\epsilon}$ 的基础上，对损失函数施加了不同的时间步权重。

![不同模型中等效的时间步权重](https://img.duanyll.com/img/bff8ac8e.png)

结合考虑重参数化的影响，SD3 的损失函数时间步权重实际上很接近 EDM.

### 插值

Rectified Flow 使用线性插值构建 $\{X_t\}$ 的轨迹：

$$
X_t = tX_1+(1-t)X_0,\quad t\in[0,1]
$$

而 DDIM 和其他概率流 ODE 可以写成更通用的仿射插值形式

$$
X_t = \alpha_t X_1 + \beta_t X_0,\quad t\in[0,1]
$$

> 具体来说, 时间步均匀的球面插值可以写成
>
> $$
> X_t = \sin(\frac{t\pi}{2})X_1 + \cos(\frac{t\pi}{2})X_0,\quad t\in[0,1]
> $$
>
> 而 DDIM 相当于在球面插值的基础上缩放了时间步
>
> $$
> \begin{aligned}
>   X_t &= \alpha_tX_1+\sqrt{1-\alpha_t^2}X_0,\quad t\in[0,1] \\
>   \alpha_t &= \exp(-\frac{1}{4}a(1-t)^2-\frac{1}{2}b(1-t))\\
>   a&=19.9, \quad b=0.1
> \end{aligned}
> $$

看起来在训练之前就必须选择一种插值形式，并且在采样时也必须使用同样的插值形式，因为速度场 $v$ 是在特定的插值形式上训练的。

实际上，在一个很宽松的条件下，可以证明，使用任何插值形式训练并使用不同的插值形式采样，使用相同的训练集数据对 $(X_0,X_1)$ 经过 Rectify 过程后，（在**理想**的速度场上）采样得到的样本对 $(Z_0,Z_1)$ 是相同的。

![不同插值方式改变走过的路径但不影响端点](https://img.duanyll.com/img/43466dda.gif)

以下是一种感性的理解方式：如果两种插值过程能以一种可微的方式互相变换，意味着插值的轨迹也可以按照相同的方式互相变换。考虑两条相交的 $X_t$ 轨迹 $X_0\to X_1,X_0'\to X_1'$，Rectify 过程只是在轨迹相交的点把轨迹重新连接为 $X_0\to X_1',X_1\to X_0'$ 从而避免轨迹相交，因此无论怎么平滑地变换轨迹，Rectify 之后都会得到相同的连接关系。也就是 Rectify 后的 Coupling （采样结果）与插值方式无关，只与数据集中的样本对 $(X_0,X_1)$ 有关。

形式化来说, 对于两条插值路径 $\{X_t:t\in[0,1]\}$ 和 $\{X_t':t\in[0,1]\}$, 如果存在可微分的映射

$$
\tau:[0,1]\to[0,1],\quad \phi:[0,1]\times\R^d\to\R^d
$$

并且 $\phi_t$ 可逆, 满足

$$
X_t' = \phi_t(X_{\tau_t}),\quad t\in[0,1],
$$

则称其为逐点可变换的. 如果两组插值路径 $\{X_t\}$ 和 $\{X_t'\}$ 来自相同的 Coupling, 起止点相同 $(X_0,X_1)=(X_0',X_1')$ 且逐点可变换, $\{Z_t\}$ 和 $\{Z_t'\}$ 由 Rectify 过程得到:

$$
\{Z_t\} = \operatorname{Rectify}(\{X_t\}),\quad \{Z_t'\} = \operatorname{Rectify}(\{X_t'\}),
$$

则可证明以下结论:

1. $\{Z_t\}$ 和 $\{Z_t'\}$ 可以使用同一个映射来变换
   $$
   Z_t' = \phi_t(Z_{\tau_t}),\quad t\in[0,1].
   $$
2. $\{Z_t\}$ 和 $\{Z_t'\}$ 的 Coupling 相同，即采样结果相同
   $$
   (Z_0,Z_1) = (Z_0',Z_1').
   $$
3. 设 $v_t$ 和 $v_t'$ 分别是 $\{Z_t\}$ 和 $\{Z_t'\}$ 的速度场，则
   $$
   v_t'(x)=\p_t\phi_t(\phi^{-1}_t(x))+(\nabla\phi_t(\phi^{-1}_t(x)))^\top v_{\tau_t}(\phi_t^{-1}(x))\dot{\tau}_t
   \label{v-transform}
   $$

![可以交换 Rectify 和变换操作](https://img.duanyll.com/img/ceb32621.png)

具体对仿射插值 (直线, 球面, DDIM) 来说: 对于两种仿射插值轨迹 $X_t=\alpha_t X_1+\beta_t X_0$ 和 $X_t'=\alpha_t' X_1+\beta_t' X_0$，存在标量函数 $\tau_t$ 和 $\omega_t$ 满足

$$
X_t' = \frac{1}{\omega_t}X_{\tau_t},\quad t\in[0,1]
$$

其中 $\tau_t$ 和 $\omega_t$ 是方程组

$$
\begin{aligned}
   \frac{\alpha_{\tau_t}}{\beta_{\tau_t}} &= \frac{\alpha_t'}{\beta_t'},\quad
   \omega_t=\frac{\alpha_{\tau_i}}{\alpha_t'}=\frac{\beta_{\tau_i}}{\beta_t'}, \quad t\in(0,1) \\
   \omega_0&=\omega_1=1,\quad \tau_0=0,\quad \tau_1=1
\end{aligned}
$$

对于实际使用的 $\alpha_t,\beta_t$ 一般难以写出显式的 $\tau_t$ 和 $\omega_t$，但可以通过数值方法求解。$\tau_t$ 相当于缩放时间步, $\omega_t$ 相当于缩放向量的模长.

![用数值方法解出直线, 球面插值和 DDIM 之间的变换关系](https://img.duanyll.com/img/5db5390c.png)

进一步得到适用于仿射变换的速度场变换公式 $(\ref{v-transform})$

$$
v_t'(x)=\frac{\dot{\tau}_t}{\omega_t}v_{\tau_t}(\omega_t x)-\frac{\dot{\omega}_t}{\omega_t}x
\label{v-transform-affine}
$$

从直线插值 $X_t=tX_1+(1-t)X_0$ 变换到仿射插值 $X_t=\alpha_t X_1+\beta_t X_0$ 的情况可以得到解析解

$$
\tau_t=\frac{\alpha_t}{\alpha_t+\beta_t},\quad
\omega_t=\frac{1}{\alpha_t+\beta_t}
$$

$$
v_t'(x)=\frac{\dot{\alpha}_t\beta_t-\alpha_t\dot{\beta}_t}{\alpha_t+\beta_t}v_{\tau_t}(\omega_t x)+\frac{\dot{\alpha}_t+\dot{\beta}_t}{\alpha_t+\beta_t}x
\label{v-transform-linear}
$$

{% folding 采样时使用不同插值方式得到相同结果 %}

![](https://img.duanyll.com/img/bd8b12b9.png)

这个演示实验在训练时使用线性插值, 但在采样时使用了按照 $\ref{v-transform-linear}$ 变换的球面插值速度场. 可以看到使用球面插值速度场时, 经过的路径不同, 但采样的结果是几乎一致的.

> 看起来直线插值的路径并不直? 首先是因为 Rectify 过程本身就不会产生直线轨迹,

{% endfolding %}

{% folding 减少时间步后不同插值方法的差异 %}

![左为直线插值, 右为线性插值](https://img.duanyll.com/img/e75c639c.png)

尽管理论上经过正确的速度场变换, 不同插值方式应该得到相同的结果, 然而实际应用中由于采样步数有限, ODE 离散化带来的误差会导致采样结果存在差异. 上图为仅使用 4 步 Euler 法采样的结果, 两种插值方式表现出了明显的区别.

![](https://img.duanyll.com/img/a44ed5db.png)

增加 Euler 法的步数, 两者差异逐渐收敛到零.

![Deepmind 博客中的实验](https://img.duanyll.com/img/65bbaf9d.png)

[DeepMind 博客](https://diffusionflow.github.io/) 中展示 DDIM 和 Flow Matching 采样时差异的实验也是只使用 6 步 Euler 法采样. 上面的分析表明此处展示的区别主要是由步数少造成的, 而不是本质上训练目标的不同.

{% endfolding %}

{% folding 插值方式的选择影响训练 %}

![](https://img.duanyll.com/img/8882792f.png)

如上一节介绍的, 训练时不同的插值方式相当于施加了时间步权重. 如果手动增加时间步权重项抵消插值方式不同的影响, 同时使用正确的方式在采样时变换速度场, 则可以得到相近的采样结果.

{% endfolding %}

### ODE / SDE

```{=latex}
\begin{tikzpicture}[
    block/.style={
        rectangle,
        rounded corners,
        draw=black,
        thick,
        text centered,
        minimum height=3.2cm,
        text width=7cm
    },
    connector/.style={
        midway,
        fill=white,
        inner sep=2pt
    }
]

% Define the nodes (blocks)
\node[block] (ot) {
    \textbf{OT Velocity Field}
    \vspace{0.5em}
    \hrule
    \vspace{0.5em}
    $Z_s = Z_t + v_{\text{OT}} \cdot (s-t)$
};

\node[block, right=4cm of ot] (ddim_v) {
    \textbf{DDIM Velocity Field}
    \vspace{0.5em}
    \hrule
    \vspace{0.5em}
    $Z_s = Z_t + v_{\text{DDIM}} \cdot (s-t)$
};

\node[block, below=2.5cm of ddim_v] (ddim_ode) {
    \textbf{DDIM ODE Sampler}
    \vspace{0.5em}
    \hrule
    \vspace{0.5em}
    \begin{align*}
        Z_s &= \alpha_s\hat{X}+\sigma_s\hat{\epsilon}, \\
        \text{where } \hat{X} &= \frac{Z_t-\sigma_t\hat{\epsilon}}{\alpha_t}
    \end{align*}
};

\node[block, below=2.5cm of ot] (ddpm_sde) {
    \textbf{DDPM SDE Sampler}
    \vspace{0.5em}
    \hrule
    \vspace{0.5em}
    \begin{align*}
        dZ_s &= \left[f(s)Z_s + g(s)^2 \frac{\hat{\epsilon}}{\sigma_s}\right] ds + g(s) dW_s \\
        &\text{\footnotesize (Score is } \nabla_{Z_s}\log p_s(Z_s) \approx -\hat{\epsilon}/\sigma_s\text{\footnotesize)}
    \end{align*}
};

% Draw the arrows and labels
\draw[<->, >=Latex, thick] (ot.east) -- (ddim_v.west)
    node[connector] {Affine Scaling};

\draw[<->, >=Latex, thick] (ddim_v.south) -- (ddim_ode.north)
    node[connector] {Reparameterization};

\draw[<->, >=Latex, thick] (ddim_ode.west) -- (ddpm_sde.east)
    node[connector] {Zero noise limit};

\draw[<->, >=Latex, thick] (ot.south) -- (ddpm_sde.north)
    node[connector] {Twdddie's Formula};

\end{tikzpicture}
```

结合之前的讨论, 已经可将 Rectified Flow 的速度场通过仿射缩放变换为 DDIM 的速度场, 再重参数化成常见的 DDIM ODE 采样公式 $(\ref{eq:ddim-step-deterministic})$ 形式, 随后根据 DDIM 论文中的推导就可以在 ODE 和 SDE 采样之间转换. 接下来从 Flow 速度场这一侧出发, 讨论在 Flow 或 Diffusion 中使用 SDE 采样的性质.

Flow Matching 的 ODE 形式已经很美好了: $Z_t$ 能保持 $X_t$ 的边际分布, 还能实现更少时间步的采样. 为什么要在这个过程中引入 SDE 要素呢? 一种动机是为了对抗 ODE 离散化求解的累积误差. 可以使用 SDE 引入一种 "反馈机制" 来修正误差.

{% folding 朗之万动力学 %}

$$
\d Z_{\tau} = \sigma^2\nabla\log\rho(Z_{\tau})\d \tau + \sqrt{2}\sigma\d W_\tau
$$

这是朗之万动力学的随机微分方程, 描述随机过程 $Z_{\tau}$ 在时间 $\tau$ 上的演化.

- 漂移项 $\sigma^2\nabla\log\rho(Z_{\tau}) \d \tau$ 是方程的确定性部分, 表示系统受到一个与概率密度 $\rho(Z_{\tau})$ 有关的的力的影响. $\nabla\log\rho$ 指向概率密度 $\rho$ 增加的方向. $\sigma$ 是参数控制漂移的幅度.
- 扩散项 $\sqrt{2}\sigma\d W_\tau$ 是方程的随机部分, 是一个布朗运动. $\d W_\tau$ 是一个服从正态分布的随机噪声, $\sqrt{2}\sigma_t$ 是控制噪声强度的参数. 加入随机性有助于系统脱离局部机制, 收敛到全局高概率区域.

这个朗之万动力学过程满足 Fokker-Planck 方程

$$
\frac{\p\rho}{\p\tau} = -\nabla\cdot\left(\sigma^2\rho(Z_{\tau})\nabla\log\rho(Z_{\tau})\right) + \sigma^2\Delta\rho(Z_{\tau})
$$

因此满足平稳性条件, 即 $\rho(Z_{\tau})$ 不随着时间 $\tau$ 的演化而改变.

{% endfolding %}

在标准的 RF ODE 中加入朗之万动力学项

$$
\d Z_t=v_t(Z_t)\d t+\sigma_t^2\nabla\log\rho_t(Z_{t,\tau})\d \tau + \sqrt{2}\sigma_t\d W_\tau
$$

其中 $\tau$ 是一个独立于 ODE 时间步 $t$ 的时间尺度, 我们假设在内层的朗之万动力学过程收敛后再在外层的 RF 过程前进. 加入朗之万动力学后, RF 的边际分布不改变, 但相当于引入了一种负反馈调节机制, 无偏差地促进系统向 $\rho_t$ 高的方向演化. 鉴于 RF 求解过程中, 轨迹的误差也没那么大, 可以认为朗之万动力学的时间尺度 $\tau$ 只用一步就能收敛, 将它与 $t$ 合并

$$
\d Z_t=v_t(Z_t)\d t+\sigma_t^2\nabla\log\rho_t(Z_t)\d t + \sqrt{2}\sigma_t\d W_t
\label{rf-with-langevin}
$$

![漂移项促进系统向概率密度高的方向移动](https://img.duanyll.com/img/26665c18.png)

下面的实验中训练时速度场 $v$ 处于欠拟合的状态, 发现在这个条件下, SDE 采样器的表现比 ODE 采样器更好.

![](https://img.duanyll.com/img/99a0575e.png)

为了写出 $\nabla \log\rho_t(x)$ 的具体表达式, 可以使用 [Tweedie 公式](https://en.wikipedia.org/wiki/Maurice_Tweedie#Tweedie's_formula). 当使用仿射插值 $X_t=\alpha_t X_1+\beta_t X_0$ 且 $X_0$ 服从与 $X_1$ 无关的标准正态分布时有

$$
\nabla \log\rho_t(x) = -\frac{1}{\beta_t}\mathbb{E}[X_0|X_t=x]
$$

代入 RF 速度场的定义式

$$
v_t(x) = \mathbb{E}[\dot{X}_t|X_t=x] = \mathbb{E}[\dot{\alpha}_t X_1 + \dot{\beta}_t X_0|X_t=x]
$$

可以得到

$$
\nabla \log\rho_t(x) = \frac{\alpha_tv_t(x)-\dot{\alpha}_tx}{\lambda_t\beta_t},\quad \lambda_t=\dot{\alpha}_t\beta_t-\alpha_t\dot{\beta}_t
\label{nabla-log-rho}
$$

代入 $(\ref{rf-with-langevin})$ 得到 SDE

$$
\d Z_t = \left(v_t(Z_t) + \frac{\alpha_t v_t(Z_t)-\dot{\alpha}_t Z_t}{\lambda_t\beta_t}\right)\d t + \sqrt{2}\sigma_t\d W_t
$$

使用线性插值 $\alpha_t = t$, $\beta_t = 1-t$，得到

$$
\d Z_t = \left(v_t(Z_t) + \frac{tv_t(Z_t)-Z_t}{1-t}\right)\d t + \sqrt{2}\sigma_t\d W_t
$$

而取 $\alpha_t^2+\beta_t^2=1$ 和 $\sigma_t=\sqrt{\dot{\alpha_t}/\alpha_t}$ 时得到 DDPM 的 SDE

$$
\d Z_t=2v_t(Z_t)\d t - \frac{\dot{\alpha}_t}{\alpha_t}Z_t\d t + \sqrt{2\frac{\dot{\alpha}_t}{\alpha_t}}\d W_t
$$

{% folding 增加噪声尺度的效果 %}

![](https://img.duanyll.com/img/29d4b30a.png)

反直觉的结果是增加噪声尺度 $\sigma_t$ 并没有给结果带来更多的 "随机性", 而是让结果更接近均值, 方差变小了. 原因在于在 $\nabla \log\rho_t(x)$ 的表达式 $(\ref{nabla-log-rho})$ 中, 当 $t\to 1$ 时, $\beta_t\to0$, 也就是说 $\nabla \log\rho_t(x)$ 应该发散到无穷, 但实际计算中 $\nabla \log\rho_t(x)$ 的值是有限的. 因此在 $t\to 1$ 时不适用 Tweedie 公式. 另外, $\beta_t\to 0$ 也会放大 $v_t$ 的误差.

![在图像生成中过大的噪声尺度会生成过于平滑的图像](https://img.duanyll.com/img/68c16634.png)

> [EDM 论文](https://arxiv.org/pdf/2206.00364) 的附录 E 中观察到了类似的现象, 但没有给出原因. 这里的分析也不够充分说明 "过度集中" 现象发生的原因，可能与神经网咯拟合速度场的误差有关。
>
> [Qiang Liu 组的最新工作](https://arxiv.org/pdf/2506.15864) 尝试解决这个问题。

{% endfolding %}
