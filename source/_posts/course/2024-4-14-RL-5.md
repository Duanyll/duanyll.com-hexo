---
title: 强化学习作业 5
tags:
  - 强化学习
---

## Problem 1 - 动作价值的学习与 Off-Policy

![](https://cdn.duanyll.com/img/20240414211915.png)

### Question 1

![](https://cdn.duanyll.com/img/20240414212309.png)

$$
\begin{aligned}
  Q_\pi(A,N)&=1+\frac{2}{3}Q_\pi(C,N)+\frac{1}{3}Q_\pi(C,E)=\frac{26}{3}\\
  Q_\pi(A,E)&=2+Q_\pi(B)=14
\end{aligned}
$$

### Question 2

![](https://cdn.duanyll.com/img/20240415093350.png)

对于 $(A,N,2,C,N)$

$$
Q(A,N)\larr Q(A,N)+\frac{1}{2}(2+Q(C,N)-Q(A,N))=\frac{53}{6}
$$

对于 $(C,N,3,E,E)$

$$
Q(C,N)\larr Q(C,N)+\frac{1}{2}(3+Q(E,E)-Q(C,N))=\frac{13}{2}
$$

对于 $(E,E,4,T)$

$$
Q(E,E)\larr Q(E,E)+\frac{1}{2}(4+Q(T))=\frac{5}{2}
$$

### Question 3

![](https://cdn.duanyll.com/img/20240415094206.png)

对于 $(A,N,2,C)$

$$
Q(A,N)\larr Q(A,N)+\frac{1}{2}(2+Q(C,E)-Q(A,N))=\frac{59}{6}
$$

对于 $(C,N,3,E)$

$$
Q(C,N)\larr Q(C,N)+\frac{1}{2}(3+Q(E,E)-Q(C,N))=\frac{13}{2}
$$

对于 $(E,E,4,T)$

$$
Q(E,E)\larr Q(E,E)+\frac{1}{2}(4+Q(T))=\frac{5}{2}
$$

### Question 4

![](https://cdn.duanyll.com/img/20240415094556.png)

![](https://cdn.duanyll.com/img/20240415094614.png)

$$
Q(A,N)\larr Q(A,N)+\frac{1}{2}(2+Q(C,N)-Q(A,N))=\frac{53}{6}
$$

$$
Q(C,E)\larr Q(C,E)+\frac{1}{2}(3+Q(D,N)-Q(C,E))=\frac{19}{2}
$$

$$
Q(D,N)\larr Q(D,N)+\frac{1}{2}(4+Q(T))=\frac{11}{2}
$$

### Question 5

![](https://cdn.duanyll.com/img/20240415095503.png)

因为状态价值直接与策略有关。在 Bellman 方程中，状态价值需要用策略对下一步的状态价值加权求和，$V_\pi(S)$ 与 $a$ 的分布有关，若 $a'$ 的分布不同于 $a$ 的分布，则需要用 IS 矫正才能得到正确的 $V_\pi(S)$, 否则得到的其实是 $V_{\pi'}(S)$. 而对于动作价值，$Q(S,a)$ 中 $a$ 已经选定了, $a$ 的分布不影响 $Q(S,a)$, 所以不需要使用 IS。

## Problem 2 - Q-Learning 算法的收敛性

![](https://cdn.duanyll.com/img/20240415102852.png)

![](https://cdn.duanyll.com/img/20240415102912.png)

$$
\begin{aligned}
  \Delta_t&\larr Q^*(s_t,a_t)-Q_t(s_t,a_t)\\
  F_t&\larr r_t+\max_b Q_t(s_{t+1}+b)-Q^*(s_t,a_t)
\end{aligned}
$$