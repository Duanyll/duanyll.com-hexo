---
title: 强化学习作业 7
tags:
  - 强化学习
---

## Problem 1

![](https://img.duanyll.com/img/20240504121013.png)

![](https://img.duanyll.com/img/20240504121035.png)

$\mathbb{E}_{p_\theta(s,a)}[R(s,a)]$ 表示在策略 $\pi_\theta$ 下从任意状态出发根据策略进行一步动作后的期望回报，与具体的 $s,a,t$ 等无关。$\sum_t\mathbb{E}_{p_\theta(s,a)}[R(s,a)]$ 表示在策略下从任意状态触发走 $t$ 步的期望回报，由于 $\mathbb{E}_{p_\theta(s,a)}[R(s,a)]$ 与 $t$ 无关，所以求和只是将 $t$ 个相同的期望回报相加，结果是 $t$ 个相同的期望回报的和。

![](https://img.duanyll.com/img/20240504122735.png)

$$
\begin{aligned}
    \mathbb{E}_{s\sim p_\theta(s)}\big[\mathbb{E}_{a\sim\pi_\theta}[R(s,a)]\big]&=p_\theta(s^1)\mathbb{E}_{a\sim\pi_\theta}[R(s^1,a)]+p_\theta(s^2)\mathbb{E}_{a\sim\pi_\theta}[R(s^2,a)]\\
    &=p_\theta(s^1)(\pi_\theta(a^1|s^1)R(s^1,a^1)+\pi_\theta(a^2|s^1)R(s^1,a^2))\\&+p_\theta(s^2)(\pi_\theta(a^1|s^2)R(s^2,a^1)+\pi_\theta(a^2|s^2)R(s^2,a^2))\\
    &=\frac{1}{2}(1\cdot 1+0)+\frac{1}{2}(0+1\cdot 2)\\
    &=\frac{3}{2}
\end{aligned}
$$

![](https://img.duanyll.com/img/20240504123656.png)

$$
\begin{aligned}
    p_{\theta,t=1}(s^1)&=\frac{1}{4},\\
    p_{\theta,t=1}(s^2)&=\frac{3}{4},\\
    p_{\theta,t=2}(s^1)&=\frac{3}{4},\\
    p_{\theta,t=2}(s^2)&=\frac{1}{4},\\
\end{aligned}
$$

![](https://img.duanyll.com/img/20240504132323.png)

$\mathbb{E}_{s\sim p_{\theta,t=1}(s)}\big[\mathbb{E}_{a\sim\pi_\theta}[R(s_1,a_1)]\big]$ 表示在 $t=1$ 时刻，采用 $\pi_\theta$ 策略从任意状态出发根据策略进行一步动作后的期望回报，与策略 $\pi_\theta$ 和在这个策略下 $t=1$ 时 $s_1$ 的分布有关。在本问题中取值是

$$
\mathbb{E}_{s\sim p_{\theta,t=1}(s)}\big[\mathbb{E}_{a\sim\pi_\theta}[R(s_1,a_1)]\big]=\frac{1}{4}(1\cdot 1+0)+\frac{3}{4}(0+1\cdot 2)=\frac{7}{4}
$$

![](https://img.duanyll.com/img/20240504133019.png)

$$
\mathbb{E}_{s\sim p_{\theta,t=2}(s)}\big[\mathbb{E}_{a\sim\pi_\theta}[R(s_2,a_2)]\big]=\frac{3}{4}(1\cdot 1+0)+\frac{1}{4}(0+1\cdot 2)=\frac{5}{4}
$$

![](https://img.duanyll.com/img/20240504133216.png)

## Problem 2

![](https://img.duanyll.com/img/20240504143914.png)

后向 TD(λ) 里的资格迹累积并衰减某个状态 $s$ 的出现次数，从而刻画过去出现的状态和现在的结果之间的关联程度。后向 GAE 中的资格迹也是通过将无穷级数求和展开成迭代的形式推导出来的，但是累加并衰减的对象不只是状态的出现次数。课件的 17 页将资格迹定义为 $\nabla_t$ 的累加，然后资格迹 $E_t$ 用来给优势函数 $\hat{A}_t$ 加权，也就是说后向 GAE 中的资格迹刻画了过去的现在的优势函数和过去的优势函数的关联程度，要求现在的策略梯度与过去的优势函数有关。

后向 TD(λ) 通过资格迹来利用现在的信息更新“过去”的状态价值。后向 GAE 里资格迹没有直接的像 TD 那样用现在的信息来更改一个现在没有直接关联的值，但也要求在求梯度的过程中，导致现在的优势函数归因到过去的动作（策略）上，这个“归因到过去”的语义是类似的。

## Problem 3

![](https://img.duanyll.com/img/20240504161002.png)

如果这里的 LM 算法是指 Levenberg-Marquardt 算法，那么这个算法是用来求解非线性最小二乘问题的。这个方法是通过将 Gauss-Newton 方法和梯度下降方法结合起来，通过引入一个参数 $\mu$ 来控制两种方法的比例。LM 方法通过求解下面的问题来获得更新步长

$$
d_k=\argmin_{d\in\R^n}\|J_kd+f_k\|_2^2+\mu\|d\|_2^2
$$

其中 $J_k$ 是 $f_k$ 的雅可比矩阵。上式相当于在牛顿法的基础上加入了阻尼项 $\mu\|d\|^2$, 对上式求偏导数并令其为零，可以得到

$$
(J_k^\top J_k+\mu I)d_k+J_k^\top f_k=0
$$

解得步长为

$$
d_k=-(J_k^\top J_k+\mu I)^{-1}J_k^\top f_k
$$

注意到 $\mu=0$ 时，上式就是 Gauss-Newton 方法的更新步长。当 $\mu$ 较大时，更新步长接近于梯度下降的更新步长。因此 LM 算法可以看作是 Gauss-Newton 方法和梯度下降方法的折中，可以看作是一种类似于信赖域思想的改进。在本节介绍的 Advanced PG 方法中，可以类比于 TRPO 对 KL 散度的约束，LM 算法对梯度的约束，通过引入 $\mu$ 来控制梯度的大小，从而保证梯度的更新不会太大，保证了更新的稳定性。TRPO 方法则约束每步更新 $\pi$ 的 KL 散度，保证了策略更新的稳定性，也通过泰勒展开来求解更新步长。

## Problem 4

![](https://img.duanyll.com/img/20240504161040.png)

从定义上看，Value-based RL 指的是通过学习值函数，学习每个状态的价值，再通过贪心方法得到贪心策略的方法，策略作为贪心的 argmax 的结果是具有确定性的。而 Policy-based RL 则跳过了学习值函数这一步，直接学习策略，因此有可能表示一个非确定性的策略。

在使用显式的表格法的时候，两种模式是容易区分的；而在使用隐式的函数近似的时候，这两种方法的区别就不明显了。如果使用隐式的方法来表示状态、动作的价值（比如 DDPG 试图解决的连续动作的情况），这个时候从状态价值贪心地得到策略就不是那么容易了，导出的策略也可以是非确定性的，于是就模糊了 Value-based 和 Policy-based 的边界。另一方面，存在一些方法，比如 AlphaZero 能同时学习策略和价值。AlphaZero 的神经网络同时给出了状态下价值和策略的估计，策略用于拓展搜索树，估计的价值用于 Backup 从而改善后续的搜索策略。这种方法可以看作是 Value-based 和 Policy-based 的结合，也是一种混合方法。于是，Value-based 和 Policy-based RL 并不是对立的，而是一种方法的不同侧重点，实际中的方法可能会同时使用两种方法的优点。
