---
title: 强化学习作业 3
tags:
  - 强化学习
---

![](https://cdn.duanyll.com/img/20240317121232.png)

## Problem 1

![](https://cdn.duanyll.com/img/20240317121318.png)

对应于从 S 出发的单源最短路问题。容易得到一种最优策略

|            | $\larr$    | $\larr$    | $\larr$    |
| ---------- | ---------- | ---------- | ---------- |
| $\uparrow$ | $\uparrow$ | $\uparrow$ | $\uparrow$ |
| $\uparrow$ | $\uparrow$ | $\uparrow$ | $\uparrow$ |
| $\uparrow$ | $\uparrow$ | $\uparrow$ | $\uparrow$ |

最优策略不唯一。

## Problem 2

![](https://cdn.duanyll.com/img/20240317121858.png)

构建 $\mathcal{R}, \mathcal{P}$ 矩阵

```mathematica
n = 4;
m = 4;
id[i_, j_] := m (i - 1) + j;
row[idx_] := Quotient[idx - 1, m] + 1;
col[idx_] := Mod[idx - 1, m] + 1;
```

```mathematica
pfunc[a_, s_, t_] := Switch[a,
   1, If[row[s, m] == 1, If[t == s, 1, 0], 
    If[t == id[row[s] - 1, col[s]], 1, 0]],
   2, If[row[s, m] == n, If[t == s, 1, 0], 
    If[t == id[row[s] + 1, col[s]], 1, 0]],
   3, If[col[s, m] == 1, If[t == s, 1, 0], 
    If[t == id[row[s], col[s] - 1], 1, 0]],
   4, If[col[s, m] == m, If[t == s, 1, 0], 
    If[t == id[row[s], col[s] + 1], 1, 0]]
   ];
rfunc [a_, s_] := If[pfunc[a, s, 1] == 1, 0, -1];
r = Table[rfunc[a, s], {a, 4}, {s, n m}];
p = Table[pfunc[a, s, t], {a, 4}, {s, n m}, {t, n m}];
```

价值迭代算法

```mathematica
valueIteration[r_, p_, v0_, \[Epsilon]_, \[Gamma]_] := 
  Module[{k, v, vlast},
   k = 0;
   v = v0;
   While[k == 0 || Norm[v - vlast] > \[Epsilon],
    vlast = v;
    v = Table[
      Max[Table[
        r[[a, s]] + \[Gamma] p[[a, s]] . vlast, {a, Length[r]}]], {s, 
       Length[v0]}];
    Print[k, v];
    k = k + 1;
    ];
   Print["Converged in ", k, " iterations"];
   Return[v];
   ];
```

运行算法，得到输出

```mathematica
v = valueIteration[r, p, ConstantArray[0, n m] , 0.1, 1]
```

![](https://cdn.duanyll.com/img/20240317151607.png)

贪心选择策略 $\pi$

```mathematica
greedyPolicy[r_, p_, v_, \[Gamma]_] := 
  Table[Ordering[
     Table[r[[a, s]] + \[Gamma] p[[a, s]] . v, {a, 
       Length[r]}], -1][[1]], {s, Length[v]}];
printPolicyTable[\[Pi]star_] := 
  ArrayReshape[{"\[UpArrow]", "\[DownArrow]", "\[LeftArrow]", 
      "\[RightArrow]"}[[\[Pi]star]], {n, m}] // MatrixForm;
```

> 1. 运行了 6 轮
> 2. 第三轮得到的价值表为 `{0,0,-1,-2,0,-1,-2,-3,-1,-2,-3,-3,-2,-3,-3,-3}`, 对应的策略为
> 
>    ![](https://cdn.duanyll.com/img/20240317151857.png)
> 3. 最后得到的价值表为 `{0,0,-1,-2,0,-1,-2,-3,-1,-2,-3,-4,-2,-3,-4,-5}`
> 4. 最后的最佳策略为
>
>    ![](https://cdn.duanyll.com/img/20240317152024.png)
>
>    与第一问得到的结果几乎是一致的。

## Problem 3

![](https://cdn.duanyll.com/img/20240317152119.png)

```mathematica
policyIteration[r_, p_, \[Pi]0_, \[Epsilon]_, \[Gamma]_, n_] := 
  Module[{k, v, \[Pi], \[Pi]last},
   k = 0;
   \[Pi] = \[Pi]0;
   v = ConstantArray[0, Length[\[Pi]0]];
   While[k == 0 || Norm[\[Pi] - \[Pi]last] > \[Epsilon],
    Do[v = 
      Table[r[[\[Pi][[s]], s]] + \[Gamma] p[[\[Pi][[s]], s]] . v, {s, 
        Length[\[Pi]0]}], n];
    \[Pi]last = \[Pi];
    \[Pi] = 
     Table[Ordering[
        Table[r[[a, s]] + \[Gamma] p[[a, s]] . v, {a, 
          Length[r]}], -1][[1]], {s, Length[v]}];
    Print["k = ", k];
    Print["v = ", v];
    Print["\[Pi] = ", \[Pi]];
    k = k + 1;
    ];
   Print["Converged in ", k, " iterations"];
   Return[\[Pi]];
   ];
```

```mathematica
policyIteration[r, p, ConstantArray[1, n m], 0.1, 1, 2]
```

![](https://cdn.duanyll.com/img/20240317155613.png)

> 1. 需要 5 轮收敛
> 2. 收敛得到的策略与 VI 一致。
> 3. 收敛得到的价值表与 VI 一致。
> 4. 第三轮结束时得到的价值表 `{0,0,-1,-6,0,-1,-2,-6,-1,-2,-3,-6,-2,-3,-6,-6}`, 对应策略表
>
>    ![](https://cdn.duanyll.com/img/20240317155905.png)

## Problem 4

![](https://cdn.duanyll.com/img/20240317155952.png)

注意到 VI 可以等价为 $N=1$ 的 PI。压缩映射原理保证了直接迭代最优价值函数能收敛，于是 VI 采取了简单直接的迭代做法。PI 则是利用价值来改进策略，但仍然需要用策略来更新对价值的估计。取有限 $N$ 的 PI 相当于是在在价值的迭代和策略的迭代之间做出了折衷。

很难比较两种做法的效率，VI 的迭代过程虽然更简单，但可能面临精确步长法相比于共轭梯度法的问题，导致总共的迭代次数比 PI 更多。